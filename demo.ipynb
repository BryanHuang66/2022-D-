{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./attachment3.csv',header=None,names=range(12))\n",
    "df.fillna(-1,inplace=True)\n",
    "df = df.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 607/607 [00:00<00:00, 1813.74it/s]\n"
     ]
    }
   ],
   "source": [
    "N = len(df) # 节点个数\n",
    "M = np.zeros([N, N])\n",
    "for i in tqdm(range(N)):\n",
    "    temp = df.iloc[i,:].to_list()\n",
    "    for j in range(N):\n",
    "        if j in temp  and i != j:\n",
    "            M[i,j] = 1\n",
    "        else:\n",
    "            M[i,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(200, 200), dpi=80)\n",
    "G = nx.from_numpy_matrix(M, create_using=nx.DiGraph)\n",
    "# nx.draw(G, node_size=100,with_labels = True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# G = nx.DiGraph()\n",
    "# G.add_edges_from([(0,1),(1,2),(2,4),(2,5),(4,6),(5,6),(1,3),(5,7)])\n",
    "# N = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.draw(G,with_labels = True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查找输入点和输出点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[365]"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_ = 1000\n",
    "max_list = []\n",
    "for item in range(N):\n",
    "    frontiers = list(G.predecessors(item))\n",
    "    if len(frontiers) < max_:\n",
    "        max_list = []\n",
    "        max_ = len(frontiers)\n",
    "        max_list.append(item)\n",
    "    elif len(frontiers)==max_:\n",
    "        max_list.append(item)\n",
    "max_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 88, 360, 381, 454]"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_ = 1000\n",
    "min_list = []\n",
    "for item in range(N):\n",
    "    frontiers = nx.immediate_dominators(G,item)\n",
    "    if len(frontiers)<min_:\n",
    "        min_list = []\n",
    "        min_ = len(frontiers)\n",
    "        min_list.append(item)\n",
    "    elif len(frontiers)==min_:\n",
    "        min_list.append(item)\n",
    "min_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置输入点为N，输出点为N+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start: N+1\n",
    "for item in max_list:\n",
    "    G.add_edges_from([(N+1,item)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "## end: N\n",
    "for item in min_list:\n",
    "    G.add_edges_from([(item,N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeCDG(G,start,end,leng):\n",
    "    G.add_edges_from([(leng,start)])\n",
    "    G.add_edges_from([(end,leng+1)])\n",
    "    G.add_edges_from([(leng,leng+1)])\n",
    "    G_reverse = G.reverse()\n",
    "    # print(G_reverse.nodes)\n",
    "    # print(leng+1)\n",
    "    frontiers = nx.dominance_frontiers(G_reverse,leng+1)\n",
    "    # print(frontiers)\n",
    "    CDG = nx.DiGraph()\n",
    "    for item in frontiers:\n",
    "        frontiers_set = frontiers[item]\n",
    "        for subitem in frontiers_set:\n",
    "            if subitem == leng+1:\n",
    "                print(\"!\")\n",
    "            else:\n",
    "                CDG.add_edges_from([(subitem,item)])\n",
    "    return CDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDG = MakeCDG(G,N+1,N,N+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set(CDG.nodes())\n",
    "a.remove(N+2)\n",
    "a.remove(N+1)\n",
    "a.remove(N)\n",
    "CDG = nx.DiGraph(nx.subgraph(CDG,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nx.immediate_dominators(CDG,365))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.draw_networkx(CDG)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成CDG关联矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(607, 607)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CDG_matrix = nx.to_numpy_array(CDG,nodelist=range(N))\n",
    "CDG_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CDG_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in range(len(CDG_matrix)):\n",
    "#     if sum(CDG_matrix[item,:])>1:\n",
    "#         print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成数据关联"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('./attachment2.csv',header=None,names=range(37))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>W</td>\n",
       "      <td>X0</td>\n",
       "      <td>X1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>604</td>\n",
       "      <td>R</td>\n",
       "      <td>X122</td>\n",
       "      <td>X123</td>\n",
       "      <td>X694</td>\n",
       "      <td>X604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>605</td>\n",
       "      <td>W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>605</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>606</td>\n",
       "      <td>W</td>\n",
       "      <td>X90</td>\n",
       "      <td>X91</td>\n",
       "      <td>X93</td>\n",
       "      <td>X96</td>\n",
       "      <td>X97</td>\n",
       "      <td>X100</td>\n",
       "      <td>X112</td>\n",
       "      <td>X356</td>\n",
       "      <td>...</td>\n",
       "      <td>X349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>606</td>\n",
       "      <td>R</td>\n",
       "      <td>X94</td>\n",
       "      <td>X95</td>\n",
       "      <td>X98</td>\n",
       "      <td>X99</td>\n",
       "      <td>X55</td>\n",
       "      <td>X76</td>\n",
       "      <td>X333</td>\n",
       "      <td>X692</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1214 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1     2     3     4     5    6     7     8     9   ...    27   28  \\\n",
       "0       0  W    X0    X1   NaN   NaN  NaN   NaN   NaN   NaN  ...   NaN  NaN   \n",
       "1       0  R   NaN   NaN   NaN   NaN  NaN   NaN   NaN   NaN  ...   NaN  NaN   \n",
       "2       1  W   NaN   NaN   NaN   NaN  NaN   NaN   NaN   NaN  ...   NaN  NaN   \n",
       "3       1  R   NaN   NaN   NaN   NaN  NaN   NaN   NaN   NaN  ...   NaN  NaN   \n",
       "4       2  W   NaN   NaN   NaN   NaN  NaN   NaN   NaN   NaN  ...   NaN  NaN   \n",
       "...   ... ..   ...   ...   ...   ...  ...   ...   ...   ...  ...   ...  ...   \n",
       "1209  604  R  X122  X123  X694  X604  NaN   NaN   NaN   NaN  ...   NaN  NaN   \n",
       "1210  605  W   NaN   NaN   NaN   NaN  NaN   NaN   NaN   NaN  ...   NaN  NaN   \n",
       "1211  605  R   NaN   NaN   NaN   NaN  NaN   NaN   NaN   NaN  ...   NaN  NaN   \n",
       "1212  606  W   X90   X91   X93   X96  X97  X100  X112  X356  ...  X349  NaN   \n",
       "1213  606  R   X94   X95   X98   X99  X55   X76  X333  X692  ...   NaN  NaN   \n",
       "\n",
       "       29   30   31   32   33   34   35   36  \n",
       "0     NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1     NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2     NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3     NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4     NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "1209  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1210  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1211  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1212  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1213  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[1214 rows x 37 columns]"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1214/1214 [00:00<00:00, 5370.87it/s]\n"
     ]
    }
   ],
   "source": [
    "write_dict = defaultdict(list)\n",
    "read_dict = defaultdict(list)\n",
    "write_var_dict = defaultdict(list)\n",
    "read_var_dict = defaultdict(list)\n",
    "for i in tqdm(range(len(df2))):\n",
    "    temp = df2.iloc[i,:].to_list()\n",
    "    if temp[1] == 'W':\n",
    "        for j in range(2,len(temp)):\n",
    "            if str(temp[j]) != \"nan\":\n",
    "                write_dict[temp[0]].append(temp[j])\n",
    "                write_var_dict[temp[j]].append(temp[0])\n",
    "            else:\n",
    "                break\n",
    "    elif temp[1] == 'R':\n",
    "        for j in range(2,len(temp)):\n",
    "            if str(temp[j]) != \"nan\":\n",
    "                read_dict[temp[0]].append(temp[j])\n",
    "                read_var_dict[temp[j]].append(temp[0])\n",
    "            else:\n",
    "                break     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 写+*约束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_G = nx.DiGraph()\n",
    "for i in range(N):\n",
    "    write_G.add_node(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 360/360 [00:02<00:00, 150.37it/s]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(write_dict):\n",
    "    for subitem in write_dict[item]:\n",
    "        for another_point in write_var_dict[subitem]+read_var_dict[subitem]:\n",
    "            if item != another_point:\n",
    "                if nx.has_path(G,item,another_point):\n",
    "                    write_G.add_edges_from([(item,another_point)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_matrix = nx.to_numpy_array(write_G,nodelist=range(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(607, 607)"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读+写约束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_G = nx.DiGraph()\n",
    "for i in range(N):\n",
    "    read_G.add_node(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373/373 [00:01<00:00, 302.64it/s]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(read_dict):\n",
    "    for subitem in read_dict[item]:\n",
    "        for another_point in write_var_dict[subitem]:\n",
    "            if item != another_point:\n",
    "                if nx.has_path(G,item,another_point):\n",
    "                    read_G.add_edges_from([(item,another_point)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_matrix = nx.to_numpy_array(read_G,nodelist=range(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(607, 607)"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并约束矩阵\n",
    "\n",
    "权重>=2为强约束，权重=1为弱约束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "cob_constrain_matrix = np.sign(CDG_matrix+read_matrix) + write_matrix*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cob_constrain_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "cob_G = nx.from_numpy_matrix(cob_constrain_matrix,parallel_edges=False, create_using=nx.DiGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[365, 390, 407, 469, 456, 454]"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.shortest_path(CDG,max_list[0], min_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nx.dag_longest_path(cob_G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题一"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 贪心算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('./attachment1.csv',names=range(5))\n",
    "df3.fillna(-1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BLOCK</td>\n",
       "      <td>TCAM</td>\n",
       "      <td>HASH</td>\n",
       "      <td>ALU</td>\n",
       "      <td>QUALIFY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>602</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>603</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>604</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>605</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>606</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>608 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1     2    3        4\n",
       "0    BLOCK  TCAM  HASH  ALU  QUALIFY\n",
       "1        0     0     0    2        0\n",
       "2        1     0     0    0        0\n",
       "3        2     0     0    0        0\n",
       "4        3     0     0    0        0\n",
       "..     ...   ...   ...  ...      ...\n",
       "603    602     0     0    1        1\n",
       "604    603     0     0   18        8\n",
       "605    604     0     0    7        0\n",
       "606    605     0     0    0        0\n",
       "607    606     0     0   25        1\n",
       "\n",
       "[608 rows x 5 columns]"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 607/607 [00:00<00:00, 7090.87it/s]\n"
     ]
    }
   ],
   "source": [
    "N = len(df3) \n",
    "property_dict = {}\n",
    "for i in tqdm(range(1,N)):\n",
    "    temp = df3.iloc[i,:].to_list()\n",
    "    property_dict[int(temp[0])] = np.array(list(map(int, temp[1:])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = cob_G.edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_dict = {}\n",
    "for item in edges:\n",
    "    edges_dict[(item[0],item[1])] = item[2]['weight']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ancestor(G,point,clist,Num=-1):\n",
    "    # print(point)\n",
    "    ancestor_nodes = G.predecessors(point)\n",
    "    for item in ancestor_nodes:\n",
    "        if item not in clist:\n",
    "            return 0\n",
    "    for item in ancestor_nodes:\n",
    "        if clist[item]==Num and edges_dict[(item,point)]>1.5:\n",
    "            # print(item,point)\n",
    "            # print(edges_dict[(item,point)])\n",
    "            return 1\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_correct_nodes(G,point,clist):\n",
    "#     all_nodes_list = []\n",
    "#     extra_node_list = []\n",
    "#     for item in G.edges(point):\n",
    "#         # print(item)\n",
    "#         if check_ancestor(G,item[1],clist):\n",
    "#             if edges_dict[item] == 1:\n",
    "#                 all_nodes_list.append(item[1])\n",
    "#             else:\n",
    "#                 extra_node_list.append(item[1])\n",
    "#     return all_nodes_list,extra_node_list\n",
    "\n",
    "def find_correct_nodes(G,point,clist,Num):\n",
    "    all_nodes_list = []\n",
    "    extra_node_list = []\n",
    "    reversed_fork_node_list = []\n",
    "    for item in G.edges(point):\n",
    "        result = check_ancestor(G,item[1],clist,Num)\n",
    "        if result==2:\n",
    "            if edges_dict[item] == 1:\n",
    "                all_nodes_list.append(item[1])\n",
    "            else:\n",
    "                extra_node_list.append(item[1])\n",
    "        elif result==0:\n",
    "            reversed_fork_node_list.append(item[1])\n",
    "        else:\n",
    "            # print('OK')\n",
    "            extra_node_list.append(item[1])\n",
    "    return all_nodes_list,extra_node_list,reversed_fork_node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_update(added_point,temp_property_arr,point_list,Num,cob_G):\n",
    "    # print('----')\n",
    "    # print(temp_property_arr)\n",
    "    all_nodes_list = []\n",
    "    extra_nodes_list = []\n",
    "    random.shuffle(point_list)\n",
    "    for item in point_list:\n",
    "        if ((Num % 2) == 0 and TCM_even>=5 and property_dict[item][0]>0):\n",
    "            pass\n",
    "        else:\n",
    "            if ((temp_property_arr - property_dict[item]) >= 0).all() == True:\n",
    "                temp_property_arr = temp_property_arr - property_dict[item]\n",
    "                # print(temp_property_arr)\n",
    "                alloc_dict[Num].append(item)\n",
    "                have_been_alloc_list[item] = Num\n",
    "                temp_all_nodes_list,temp_extra_node_list,_ = find_correct_nodes(cob_G,item,have_been_alloc_list,Num)\n",
    "                # print(temp_all_nodes_list)\n",
    "                all_nodes_list += temp_all_nodes_list\n",
    "                extra_nodes_list += temp_extra_node_list\n",
    "                added_point.append(item)\n",
    "                # print(temp_extra_node_list)\n",
    "            \n",
    "    if all_nodes_list != []:\n",
    "        # print(added_point)\n",
    "        # print(temp_all_nodes_list)\n",
    "        temp_temp_all_nodes_list,temp_temp_extra_node_list,temp_added_point,temp_property_arr = recursive_update(added_point,temp_property_arr,all_nodes_list,Num,cob_G)\n",
    "        # print(temp_all_nodes_list)\n",
    "        # print(temp_temp_all_nodes_list)\n",
    "        extra_nodes_list += temp_temp_extra_node_list\n",
    "        all_nodes_list += temp_temp_all_nodes_list\n",
    "        added_point = temp_added_point\n",
    "\n",
    "    return all_nodes_list,extra_nodes_list,added_point,temp_property_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_recursive_update(added_point,temp_property_arr,point_list,Num,cob_G):\n",
    "    all_nodes_list = []\n",
    "    extra_nodes_list = []\n",
    "    random.shuffle(point_list)\n",
    "    for item in point_list:\n",
    "        temp_all_nodes_list = []\n",
    "        if ((Num % 2) == 0 and TCM_even>=5 and property_dict[item][0]>0):\n",
    "            pass\n",
    "        else:\n",
    "            if ((temp_property_arr - property_dict[item]) >= 0).all() == True:\n",
    "                temp_property_arr = temp_property_arr - property_dict[item]\n",
    "                alloc_dict[Num].append(item)\n",
    "                have_been_alloc_list[item] = Num\n",
    "                added_point.append(item)\n",
    "                temp_all_nodes_list,temp_extra_node_list,_ = find_correct_nodes(cob_G,item,have_been_alloc_list,Num)\n",
    "                all_nodes_list += temp_all_nodes_list\n",
    "                extra_nodes_list += temp_extra_node_list\n",
    "                \n",
    "            \n",
    "        if temp_all_nodes_list != []:\n",
    "            # print(temp_all_nodes_list)\n",
    "            temp_temp_all_nodes_list,temp_temp_extra_node_list,temp_added_point,temp_property_arr = tree_recursive_update(added_point,temp_property_arr,temp_all_nodes_list,Num,cob_G)\n",
    "            extra_nodes_list += temp_temp_extra_node_list\n",
    "            all_nodes_list += temp_temp_all_nodes_list\n",
    "            added_point = temp_added_point\n",
    "    return all_nodes_list,extra_nodes_list,added_point,temp_property_arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_limit_arr = np.zeros((1000,4))\n",
    "point_list = [start_point]\n",
    "Num = 0  ## 层级\n",
    "TCM_even = 0  ## 偶数数量\n",
    "alloc_dict = defaultdict(list)\n",
    "have_been_alloc_list = {}\n",
    "TCM_even_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:00<00:03, 28.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 44.84it/s]\n"
     ]
    }
   ],
   "source": [
    "mini_ = 100\n",
    "for i in tqdm(range(100)):\n",
    "    acc_limit_arr = np.zeros((1000,4))\n",
    "    point_list = [start_point]\n",
    "    Num = 0  ## 层级\n",
    "    TCM_even = 0  ## 偶数数量\n",
    "    alloc_dict = defaultdict(list)\n",
    "    have_been_alloc_list = {}\n",
    "    TCM_even_list = []\n",
    "    while True:\n",
    "        extra_point_list = []\n",
    "        if  16 <= Num and Num<= 31:\n",
    "            temp_property_arr =np.minimum(np.array([1,3,56+acc_limit_arr[Num-16,2],64+acc_limit_arr[Num-16,3]]) - acc_limit_arr[Num-16,:],np.array([1,2,56,64]))\n",
    "            orig_arr = temp_property_arr\n",
    "        else:\n",
    "            temp_property_arr = np.array([1,2,56,64])\n",
    "            orig_arr = temp_property_arr\n",
    "        # print(temp_property_arr)\n",
    "        # print(point_list)\n",
    "        # print(have_been_alloc_list)\n",
    "        temp_all_nodes_list,temp_extra_node_list,added_point,temp_property_arr = recursive_update([],temp_property_arr,point_list,Num,cob_G)\n",
    "        point_list =  temp_all_nodes_list + temp_extra_node_list + point_list\n",
    "        # print(point_list)\n",
    "        for item in added_point:\n",
    "            point_list.remove(item)\n",
    "        acc_limit_arr[Num,:] =  orig_arr-temp_property_arr\n",
    "        \n",
    "        \n",
    "        if (Num % 2) == 0 and temp_property_arr[0]== 0 and Num-16 not in TCM_even_list:\n",
    "            TCM_even += 1\n",
    "            TCM_even_list.append(Num)\n",
    "        # print(f'{Num} 级共有 {len(alloc_dict[Num])} 个block')\n",
    "        Num += 1\n",
    "        \n",
    "        if point_list == []:\n",
    "            break\n",
    "    if Num<mini_:\n",
    "        mini_ = Num\n",
    "        print(mini_)\n",
    "        df4 = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in alloc_dict.items() ]))\n",
    "        df4 = df4.transpose()\n",
    "        # df4.drop(index=0)\n",
    "        df4.to_csv('first_problem_w_greedy.csv', header=False)\n",
    "        pd.DataFrame(acc_limit_arr).to_csv(\"first_problem_w_greedy_alloc.csv\", header=False)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in alloc_dict.items() ]))\n",
    "df4 = df4.transpose()\n",
    "# df4.drop(index=0)\n",
    "df4.to_csv('first_problem_w_greedy_tree.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(acc_limit_arr).to_csv(\"first_problem_w_greedy_tree_alloc.csv\", header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def multi_head_tree_recursive_update(added_point,property_arr,point_list,Num,cob_G,reversed_fork_node_dict):\n",
    "    # specail_nodes_list = []\n",
    "    all_nodes_list = []\n",
    "    extra_nodes_list = []\n",
    "    temp_property_arr = deepcopy(property_arr)\n",
    "    random.shuffle(point_list)\n",
    "    for item in point_list:\n",
    "\n",
    "        temp_all_nodes_list = []\n",
    "\n",
    "        temp_property_arr[1] = property_arr[1]\n",
    "        temp_property_arr[2] = property_arr[2]\n",
    "        if ((Num % 2) == 0 and TCM_even>=5 and property_dict[item][0]>0):\n",
    "            pass\n",
    "        else:\n",
    "            if ((temp_property_arr - property_dict[item]) >= 0).all() == True:\n",
    "                \n",
    "                temp_property_arr = temp_property_arr - property_dict[item]\n",
    "                if item in reversed_fork_node_dict:\n",
    "                    picked_out_property_arr = reversed_fork_node_dict[item]\n",
    "                    new_property_arr = picked_out_property_arr - property_dict[item]\n",
    "                    if (new_property_arr>=0).all() == True:\n",
    "                        temp_property_arr = np.minimum(temp_property_arr,new_property_arr)\n",
    "                        alloc_dict[Num].append(item)\n",
    "                        have_been_alloc_list[item] = Num\n",
    "                        added_point.append(item)\n",
    "                        temp_all_nodes_list,temp_extra_node_list,temp_reversed_fork_node_list = find_correct_nodes(cob_G,item,have_been_alloc_list,Num)\n",
    "                        all_nodes_list += temp_all_nodes_list\n",
    "                        extra_nodes_list += temp_extra_node_list\n",
    "                        for subitem in temp_reversed_fork_node_list:\n",
    "                            if subitem in reversed_fork_node_dict:\n",
    "                                reversed_fork_node_dict[subitem] = np.minimum(temp_property_arr,reversed_fork_node_dict[subitem])  \n",
    "                            else:\n",
    "                                reversed_fork_node_dict[subitem] = temp_property_arr\n",
    "                        \n",
    "                    else:\n",
    "                        temp_property_arr = temp_property_arr + property_dict[item]\n",
    "                        \n",
    "                else:\n",
    "                    # print(3)\n",
    "                    alloc_dict[Num].append(item)\n",
    "                    have_been_alloc_list[item] = Num\n",
    "                    added_point.append(item)\n",
    "                    temp_all_nodes_list,temp_extra_node_list,temp_reversed_fork_node_list = find_correct_nodes(cob_G,item,have_been_alloc_list,Num)\n",
    "                    all_nodes_list += temp_all_nodes_list\n",
    "                    extra_nodes_list += temp_extra_node_list\n",
    "                    for subitem in temp_reversed_fork_node_list:\n",
    "                        if subitem in reversed_fork_node_dict:\n",
    "                                reversed_fork_node_dict[subitem] = np.minimum(temp_property_arr,reversed_fork_node_dict[subitem])  \n",
    "                        else:\n",
    "                            reversed_fork_node_dict[subitem] = temp_property_arr\n",
    " \n",
    "        if temp_all_nodes_list != []:\n",
    "            temp_temp_all_nodes_list,temp_temp_extra_node_list,temp_added_point,temp_property_arr,reversed_fork_node_dict = multi_head_tree_recursive_update(added_point,temp_property_arr,temp_all_nodes_list,Num,cob_G,reversed_fork_node_dict)\n",
    "            extra_nodes_list += temp_temp_extra_node_list\n",
    "            all_nodes_list += temp_temp_all_nodes_list\n",
    "            added_point = temp_added_point\n",
    "\n",
    "    return all_nodes_list,extra_nodes_list,added_point,temp_property_arr,reversed_fork_node_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_limit_arr = np.zeros((1000,4))\n",
    "point_list = [start_point]\n",
    "Num = 0  ## 层级\n",
    "TCM_even = 0  ## 偶数数量\n",
    "alloc_dict = defaultdict(list)\n",
    "have_been_alloc_list = {}\n",
    "TCM_even_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:00<00:03, 12.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 32.47it/s]\n"
     ]
    }
   ],
   "source": [
    "mini_ = 100\n",
    "for i in tqdm(range(50)):\n",
    "    acc_limit_arr = np.zeros((10000,4))\n",
    "    point_list = [start_point]\n",
    "    Num = 0  ## 层级\n",
    "    TCM_even = 0  ## 偶数数量\n",
    "    alloc_dict = defaultdict(list)\n",
    "    have_been_alloc_list = {}\n",
    "    TCM_even_list = []\n",
    "    all_len = 0\n",
    "    while True:\n",
    "        extra_point_list = []\n",
    "        if  16 <= Num and Num<= 31:\n",
    "            temp_property_arr =np.minimum(np.array([1,3,56+acc_limit_arr[Num-16,2],64+acc_limit_arr[Num-16,3]]) - acc_limit_arr[Num-16,:],np.array([1,2,56,64]))\n",
    "            orig_arr = temp_property_arr\n",
    "        else:\n",
    "            temp_property_arr = np.array([1,2,56,64])\n",
    "            orig_arr = temp_property_arr\n",
    "        # if Num==29:\n",
    "        #     print(temp_property_arr)\n",
    "        # print(temp_property_arr)\n",
    "        # print(point_list)\n",
    "        # print(have_been_alloc_list)\n",
    "        temp_all_nodes_list,temp_extra_node_list,added_point,temp_property_arr,reversed_fork_node_dict = multi_head_tree_recursive_update([],temp_property_arr,point_list,Num,cob_G,{})\n",
    "        # print(point_list)\n",
    "        # print(temp_all_nodes_list)\n",
    "        point_list =  temp_all_nodes_list + temp_extra_node_list + point_list\n",
    "        # print(reversed_fork_node_dict)\n",
    "        # print(point_list)\n",
    "        for item in added_point:\n",
    "            point_list.remove(item)\n",
    "        acc_limit_arr[Num,:] =  orig_arr-temp_property_arr\n",
    "        \n",
    "        \n",
    "        if (Num % 2) == 0 and temp_property_arr[0]== 0 and Num-16 not in TCM_even_list:\n",
    "            TCM_even += 1\n",
    "            TCM_even_list.append(Num)\n",
    "        # print(f'{Num} 级共有 {len(alloc_dict[Num])} 个block')\n",
    "        all_len = all_len + len(alloc_dict[Num])\n",
    "        Num += 1\n",
    "        \n",
    "        if point_list == []:\n",
    "            break\n",
    "        # if Num==29:\n",
    "        #     break\n",
    "    if Num<mini_:\n",
    "        mini_ = Num\n",
    "        print(mini_)\n",
    "        df5 = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in alloc_dict.items() ]))\n",
    "        df5 = df5.transpose()\n",
    "        # df4.drop(index=0)\n",
    "        df5.to_csv('second_problem_w_greedy_tree.csv', header=False)\n",
    "        pd.DataFrame(acc_limit_arr).to_csv(\"second_problem_w_greedy_alloc.csv\", header=False)\n",
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in alloc_dict.items() ]))\n",
    "df5 = df5.transpose()\n",
    "# df4.drop(index=0)\n",
    "df5.to_csv('second_problem_w_greedy_tree.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(acc_limit_arr).to_csv(\"second_problem_w_greedy_tree_alloc.csv\", header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 权值搜索 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从后向前计算各个节点的property：（之后节点数，TCAM，HASH，ALU，QUALIFY）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 607/607 [00:00<00:00, 1467.48it/s]\n"
     ]
    }
   ],
   "source": [
    "bp_property_dict = {}\n",
    "\n",
    "for item in tqdm(range(N-1)):\n",
    "    all_points = nx.descendants(cob_G,item)\n",
    "    cal_property = deepcopy(property_dict[item])\n",
    "    for subitem in all_points:\n",
    "        cal_property += deepcopy(property_dict[subitem])\n",
    "    bp_property_dict[item] = [len(all_points)+1,cal_property]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_update_w_right(added_point,temp_property_arr,point_list,Num,cob_G):\n",
    "    # print('----')\n",
    "    # print(temp_property_arr)\n",
    "    all_nodes_list = []\n",
    "    extra_nodes_list = []\n",
    "    new_dict_list = {}\n",
    "    for item in point_list:\n",
    "        new_dict_list[item] = property_dict[item]\n",
    "    point_list = dict(sorted(new_dict_list.items(), key=lambda item: item[1][1], reverse=True)).keys()\n",
    "    \n",
    "    for item in point_list:\n",
    "        if ((Num % 2) == 0 and TCM_even>=5 and property_dict[item][0]>0):\n",
    "            pass\n",
    "        else:\n",
    "            if ((temp_property_arr - property_dict[item]) >= 0).all() == True:\n",
    "                temp_property_arr = temp_property_arr - property_dict[item]\n",
    "                # print(temp_property_arr)\n",
    "                alloc_dict[Num].append(item)\n",
    "                have_been_alloc_list[item] = Num\n",
    "                temp_all_nodes_list,temp_extra_node_list,_ = find_correct_nodes(cob_G,item,have_been_alloc_list,Num)\n",
    "                # print(temp_all_nodes_list)\n",
    "                all_nodes_list += temp_all_nodes_list\n",
    "                extra_nodes_list += temp_extra_node_list\n",
    "                added_point.append(item)\n",
    "                # print(temp_extra_node_list)\n",
    "            \n",
    "    if all_nodes_list != []:\n",
    "        # print(added_point)\n",
    "        # print(temp_all_nodes_list)\n",
    "        temp_temp_all_nodes_list,temp_temp_extra_node_list,temp_added_point,temp_property_arr = recursive_update_w_right(added_point,temp_property_arr,all_nodes_list,Num,cob_G)\n",
    "        # print(temp_all_nodes_list)\n",
    "        # print(temp_temp_all_nodes_list)\n",
    "        extra_nodes_list += temp_temp_extra_node_list\n",
    "        all_nodes_list += temp_temp_all_nodes_list\n",
    "        added_point = temp_added_point\n",
    "\n",
    "    return all_nodes_list,extra_nodes_list,added_point,temp_property_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mini_ = 100\n",
    "# for i in tqdm(range(100)):\n",
    "acc_limit_arr = np.zeros((1000,4))\n",
    "point_list = [start_point]\n",
    "Num = 0  ## 层级\n",
    "TCM_even = 0  ## 偶数数量\n",
    "alloc_dict = defaultdict(list)\n",
    "have_been_alloc_list = {}\n",
    "TCM_even_list = []\n",
    "while True:\n",
    "    extra_point_list = []\n",
    "    if  16 <= Num and Num<= 31:\n",
    "        temp_property_arr =np.minimum(np.array([1,3,56+acc_limit_arr[Num-16,2],64+acc_limit_arr[Num-16,3]]) - acc_limit_arr[Num-16,:],np.array([1,2,56,64]))\n",
    "        orig_arr = temp_property_arr\n",
    "    else:\n",
    "        temp_property_arr = np.array([1,2,56,64])\n",
    "        orig_arr = temp_property_arr\n",
    "    # print(temp_property_arr)\n",
    "    # print(point_list)\n",
    "    # print(have_been_alloc_list)\n",
    "    temp_all_nodes_list,temp_extra_node_list,added_point,temp_property_arr = recursive_update_w_right([],temp_property_arr,point_list,Num,cob_G)\n",
    "    point_list =  temp_all_nodes_list + temp_extra_node_list + point_list\n",
    "    # print(point_list)\n",
    "    for item in added_point:\n",
    "        point_list.remove(item)\n",
    "    acc_limit_arr[Num,:] =  orig_arr-temp_property_arr\n",
    "    \n",
    "    \n",
    "    if (Num % 2) == 0 and temp_property_arr[0]== 0 and Num-16 not in TCM_even_list:\n",
    "        TCM_even += 1\n",
    "        TCM_even_list.append(Num)\n",
    "    # print(f'{Num} 级共有 {len(alloc_dict[Num])} 个block')\n",
    "    Num += 1\n",
    "    \n",
    "    if point_list == []:\n",
    "        break\n",
    "# if Num<mini_:\n",
    "#     mini_ = Num\n",
    "    # print(mini_)\n",
    "df4 = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in alloc_dict.items() ]))\n",
    "df4 = df4.transpose()\n",
    "# df4.drop(index=0)\n",
    "df4.to_csv('first_problem_w_greedy_w_right.csv', header=False)\n",
    "pd.DataFrame(acc_limit_arr).to_csv(\"first_problem_w_greedy_alloc_w_right.csv\", header=False)\n",
    "    # break\n",
    "Num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 限制资源算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_update_w_forbid_prior(added_point,temp_property_arr,point_list,Num,cob_G):\n",
    "    all_nodes_list = []\n",
    "    extra_nodes_list = []\n",
    "    problem_report = []\n",
    "    # random.shuffle(point_list)\n",
    "    for item in point_list:\n",
    "        if ((Num % 2) == 0 and TCM_even>=5 and property_dict[item][0]>0):\n",
    "            pass\n",
    "        else:\n",
    "            if ((temp_property_arr - property_dict[item]) >= 0).all() == True:\n",
    "                temp_property_arr = temp_property_arr - property_dict[item]\n",
    "                # print(temp_property_arr)\n",
    "                alloc_dict[Num].append(item)\n",
    "                have_been_alloc_list[item] = Num\n",
    "                temp_all_nodes_list,temp_extra_node_list,_ = find_correct_nodes(cob_G,item,have_been_alloc_list,Num)\n",
    "                # print(temp_all_nodes_list)\n",
    "                all_nodes_list += temp_all_nodes_list\n",
    "                extra_nodes_list += temp_extra_node_list\n",
    "                added_point.append(item)\n",
    "                # print(temp_extra_node_list)\n",
    "            else:\n",
    "                problem_report = [item,((temp_property_arr - property_dict[item]))]\n",
    "            \n",
    "    if all_nodes_list != []:\n",
    "        # print(all_nodes_list)\n",
    "        # print(added_point)\n",
    "        # print(temp_all_nodes_list)\n",
    "        temp_temp_all_nodes_list,temp_temp_extra_node_list,temp_added_point,temp_property_arr,problem_report = recursive_update_w_forbid_prior(added_point,temp_property_arr,all_nodes_list,Num,cob_G)\n",
    "        # print(temp_all_nodes_list)\n",
    "        # print(temp_temp_all_nodes_list)\n",
    "        extra_nodes_list += temp_temp_extra_node_list\n",
    "        all_nodes_list += temp_temp_all_nodes_list\n",
    "        added_point = temp_added_point\n",
    "\n",
    "    return all_nodes_list,extra_nodes_list,added_point,temp_property_arr,problem_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:00<00:02, 42.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 56.28it/s]\n"
     ]
    }
   ],
   "source": [
    "mini_ = 100\n",
    "prperty_alloc_arr = np.repeat([[1,2,56,64]],16,axis=0)\n",
    "for i in tqdm(range(100)):\n",
    "    acc_limit_arr = np.zeros((1000,4))\n",
    "    point_list = [start_point]\n",
    "    Num = 0  ## 层级\n",
    "    TCM_even = 0  ## 偶数数量\n",
    "    alloc_dict = defaultdict(list)\n",
    "    have_been_alloc_list = {}\n",
    "    TCM_even_list = []\n",
    "    while True:\n",
    "        extra_point_list = []\n",
    "        if Num<16:\n",
    "            temp_property_arr = prperty_alloc_arr[Num,:]\n",
    "        elif  16 <= Num and Num<= 31:\n",
    "            temp_property_arr =np.array([1,3,56+acc_limit_arr[Num-16,2],64+acc_limit_arr[Num-16,3]]) - acc_limit_arr[Num-16,:]\n",
    "            orig_arr = temp_property_arr\n",
    "        else:\n",
    "            temp_property_arr = np.array([1,2,56,64])\n",
    "            orig_arr = temp_property_arr\n",
    "        temp_all_nodes_list,temp_extra_node_list,added_point,temp_property_arr,report = recursive_update_w_forbid_prior([],temp_property_arr,point_list,Num,cob_G)\n",
    "        point_list =  temp_all_nodes_list + temp_extra_node_list + point_list\n",
    "\n",
    "        for item in added_point:\n",
    "            point_list.remove(item)\n",
    "        acc_limit_arr[Num,:] =  orig_arr-temp_property_arr\n",
    "        \n",
    "        \n",
    "        if (Num % 2) == 0 and temp_property_arr[0]== 0 and Num-16 not in TCM_even_list:\n",
    "            TCM_even += 1\n",
    "            TCM_even_list.append(Num)\n",
    "        # print(f'{Num} 级共有 {len(alloc_dict[Num])} 个block')\n",
    "      \n",
    "        \n",
    "        if 20 <= Num and Num<= 31 and len(alloc_dict[Num])<6:\n",
    "            if report != []:\n",
    "                ran_num = random.random()\n",
    "                if ran_num<1/2:\n",
    "                    prperty_alloc_arr[Num-16] =np.maximum(np.minimum(np.array([1,3,56,64]) - acc_limit_arr[Num,:]+report[1],np.array([1,2,56,64])),np.array([1,1,0,0]))\n",
    "                else:\n",
    "                    prperty_alloc_arr[Num-16] = np.array([1,2,56,64])\n",
    "            # else:\n",
    "            #     prperty_alloc_arr[Num-16] =np.maximum(np.minimum(np.array([1,3,56,64]) - acc_limit_arr[Num,:],np.array([1,1,28,32])),np.zeros(4))\n",
    "        elif 16 <= Num and Num < 20:\n",
    "            prperty_alloc_arr[Num-16] = np.array([1,2,56,64])\n",
    "        Num += 1\n",
    "        if point_list == []:\n",
    "            break\n",
    "    # print(Num)\n",
    "    if Num<mini_:\n",
    "        mini_ = Num\n",
    "        print(mini_)\n",
    "        df4 = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in alloc_dict.items() ]))\n",
    "        df4 = df4.transpose()\n",
    "        # df4.drop(index=0)\n",
    "        df4.to_csv('first_problem_w_greedy_limit_resource.csv', header=False)\n",
    "        pd.DataFrame(acc_limit_arr).to_csv(\"first_problem_w_greedy_limit_resource.csv\", header=False)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度广度搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "\n",
    "def cross_recursive_update(added_point,temp_property_arr,point_list,Num,cob_G,rnd):\n",
    "    all_nodes_list = []\n",
    "    extra_nodes_list = []\n",
    "    # new_dict_list = {}\n",
    "    # for item in point_list:\n",
    "    #     new_dict_list[item] = property_dict[item]\n",
    "    # point_list = dict(sorted(new_dict_list.items(), key=lambda item: item[1][0], reverse=True)).keys()\n",
    "    random.shuffle(point_list)\n",
    "    if rnd==0:\n",
    "        # print(1)\n",
    "        for item in point_list:\n",
    "            temp_all_nodes_list = []\n",
    "            if ((Num % 2) == 0 and TCM_even>=5 and property_dict[item][0]>0):\n",
    "                pass\n",
    "            else:\n",
    "                if ((temp_property_arr - property_dict[item]) >= 0).all() == True:\n",
    "                    temp_property_arr = temp_property_arr - property_dict[item]\n",
    "                    alloc_dict[Num].append(item)\n",
    "                    have_been_alloc_list[item] = Num\n",
    "                    added_point.append(item)\n",
    "                    temp_all_nodes_list,temp_extra_node_list,_ = find_correct_nodes(cob_G,item,have_been_alloc_list,Num)\n",
    "                    all_nodes_list += temp_all_nodes_list\n",
    "                    extra_nodes_list += temp_extra_node_list\n",
    "                    \n",
    "                \n",
    "            if temp_all_nodes_list != []:\n",
    "                # print(temp_all_nodes_list)\n",
    "                rand_num = random.random()\n",
    "                if bp_property_dict[item][0]/((N-1-len(alloc_dict))/len(point_list))>rand_num:\n",
    "                    rnd = 0\n",
    "                else:\n",
    "                    rnd = 1\n",
    "                temp_temp_all_nodes_list,temp_temp_extra_node_list,temp_added_point,temp_property_arr = cross_recursive_update(added_point,temp_property_arr,temp_all_nodes_list,Num,cob_G,rnd)\n",
    "                extra_nodes_list += temp_temp_extra_node_list\n",
    "                all_nodes_list += temp_temp_all_nodes_list\n",
    "                added_point = temp_added_point\n",
    "        # return all_nodes_list,extra_nodes_list,added_point,temp_property_arr\n",
    "    \n",
    "    else:\n",
    "        # print(2)\n",
    "        for item in point_list:\n",
    "            if ((Num % 2) == 0 and TCM_even>=5 and property_dict[item][0]>0):\n",
    "                pass\n",
    "            else:\n",
    "                if ((temp_property_arr - property_dict[item]) >= 0).all() == True:\n",
    "                    temp_property_arr = temp_property_arr - property_dict[item]\n",
    "                    # print(temp_property_arr)\n",
    "                    alloc_dict[Num].append(item)\n",
    "                    have_been_alloc_list[item] = Num\n",
    "                    temp_all_nodes_list,temp_extra_node_list,_ = find_correct_nodes(cob_G,item,have_been_alloc_list,Num)\n",
    "                    # print(temp_all_nodes_list)\n",
    "                    all_nodes_list += temp_all_nodes_list\n",
    "                    extra_nodes_list += temp_extra_node_list\n",
    "                    added_point.append(item)\n",
    "                    # print(temp_extra_node_list)\n",
    "                \n",
    "        if all_nodes_list != []:\n",
    "            rand_num = random.random()\n",
    "            # print(bp_property_dict[item][0]/((N-1-len(alloc_dict))/len(point_list)))\n",
    "            # print(rand_num)\n",
    "            if bp_property_dict[item][0]/((N-1-len(alloc_dict))/len(point_list))>rand_num:\n",
    "                rnd = 0\n",
    "            else:\n",
    "                rnd = 1\n",
    "            # print(added_point)\n",
    "            # print(temp_all_nodes_list)\n",
    "            temp_temp_all_nodes_list,temp_temp_extra_node_list,temp_added_point,temp_property_arr = cross_recursive_update(added_point,temp_property_arr,all_nodes_list,Num,cob_G,randrange)\n",
    "            # print(temp_all_nodes_list)\n",
    "            # print(temp_temp_all_nodes_list)\n",
    "            extra_nodes_list += temp_temp_extra_node_list\n",
    "            all_nodes_list += temp_temp_all_nodes_list\n",
    "            added_point = temp_added_point\n",
    "\n",
    "    return all_nodes_list,extra_nodes_list,added_point,temp_property_arr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/500 [00:00<00:18, 27.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:11<00:00, 44.40it/s]\n"
     ]
    }
   ],
   "source": [
    "mini_ = 100\n",
    "Num_list = []\n",
    "for i in tqdm(range(50)):\n",
    "    acc_limit_arr = np.zeros((1000,4))\n",
    "    point_list = [start_point]\n",
    "    Num = 0  ## 层级\n",
    "    TCM_even = 0  ## 偶数数量\n",
    "    alloc_dict = defaultdict(list)\n",
    "    have_been_alloc_list = {}\n",
    "    TCM_even_list = []\n",
    "    while True:\n",
    "        extra_point_list = []\n",
    "        if  16 <= Num and Num<= 31:\n",
    "            temp_property_arr =np.minimum(np.array([1,3,56+acc_limit_arr[Num-16,2],64+acc_limit_arr[Num-16,3]]) - acc_limit_arr[Num-16,:],np.array([1,2,56,64]))\n",
    "            orig_arr = temp_property_arr\n",
    "        else:\n",
    "            temp_property_arr = np.array([1,2,56,64])\n",
    "            orig_arr = temp_property_arr\n",
    "        # print(temp_property_arr)\n",
    "        # print(point_list)\n",
    "        # print(have_been_alloc_list)\n",
    "        temp_all_nodes_list,temp_extra_node_list,added_point,temp_property_arr = cross_recursive_update([],temp_property_arr,point_list,Num,cob_G,0)\n",
    "        point_list =  temp_all_nodes_list + temp_extra_node_list + point_list\n",
    "        # print(point_list)\n",
    "        for item in added_point:\n",
    "            point_list.remove(item)\n",
    "        acc_limit_arr[Num,:] =  orig_arr-temp_property_arr\n",
    "        \n",
    "        \n",
    "        if (Num % 2) == 0 and temp_property_arr[0]== 0 and Num-16 not in TCM_even_list:\n",
    "            TCM_even += 1\n",
    "            TCM_even_list.append(Num)\n",
    "        # print(f'{Num} 级共有 {len(alloc_dict[Num])} 个block')\n",
    "        Num += 1\n",
    "        \n",
    "        if point_list == []:\n",
    "            break\n",
    "    Num_list.append(Num)\n",
    "    if Num<mini_:\n",
    "        mini_ = Num\n",
    "        print(mini_)\n",
    "        df4 = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in alloc_dict.items() ]))\n",
    "        df4 = df4.transpose()\n",
    "        # df4.drop(index=0)\n",
    "        df4.to_csv('first_problem_w_greedy_random.csv', header=False)\n",
    "        pd.DataFrame(acc_limit_arr).to_csv(\"first_problem_w_greedy_alloc_random.csv\", header=False)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单层DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def DP_head_tree_recursive_update(property_arr,point_list,Num,cob_G,reversed_fork_node_dict,reversed_fork_list_dict = defaultdict(set),cur_layer_list=defaultdict(set)):\n",
    "    # specail_nodes_list = []\n",
    "    all_nodes_list = []\n",
    "    extra_nodes_list = []\n",
    "    temp_property_arr = deepcopy(property_arr)\n",
    "    random.shuffle(point_list)\n",
    "    for item in point_list:\n",
    "\n",
    "        temp_all_nodes_list = []\n",
    "\n",
    "        ## 进行第二支树形搜索时重置资源数\n",
    "        temp_property_arr = property_arr\n",
    "        \n",
    "        if ((Num % 2) == 0 and TCM_even>=5 and property_dict[item][0]>0):\n",
    "            pass\n",
    "        else:\n",
    "            if ((temp_property_arr - property_dict[item]) >= 0).all() == True:\n",
    "                \n",
    "                temp_property_arr = temp_property_arr - property_dict[item]\n",
    "                if item in reversed_fork_node_dict:\n",
    "                    ## 汇合节点\n",
    "                    picked_out_property_arr = reversed_fork_node_dict[item]\n",
    "                    new_property_arr = picked_out_property_arr - property_dict[item]\n",
    "                    if (new_property_arr>=0).all() == True:\n",
    "                        temp = set([item])\n",
    "                        for subitem in reversed_fork_list_dict[item]:\n",
    "                            temp.update(cur_layer_list[subitem])\n",
    "                        cur_layer_list[item] = temp\n",
    "                        # print(cur_layer_list)\n",
    "                        \n",
    "                        \n",
    "                        temp_property_arr = np.minimum(temp_property_arr,new_property_arr)\n",
    "                        alloc_dict[Num].append(item)\n",
    "                        have_been_alloc_list[item] = Num\n",
    "                        \n",
    "                        temp_all_nodes_list,temp_extra_node_list,temp_reversed_fork_node_list = find_correct_nodes(cob_G,item,have_been_alloc_list,Num)\n",
    "                        all_nodes_list += temp_all_nodes_list\n",
    "                        extra_nodes_list += temp_extra_node_list\n",
    "                        ## 检查reverse_fork节点后的节点\n",
    "                        for subitem in temp_all_nodes_list:\n",
    "                            reversed_fork_list_dict[subitem].add(item)\n",
    "                        \n",
    "                        for subitem in temp_reversed_fork_node_list:\n",
    "                            if subitem in reversed_fork_node_dict:\n",
    "                                reversed_fork_node_dict[subitem] = np.minimum(temp_property_arr,reversed_fork_node_dict[subitem])  \n",
    "                                reversed_fork_list_dict[subitem].add(item)\n",
    "                            else:\n",
    "                                reversed_fork_node_dict[subitem] = temp_property_arr\n",
    "                                reversed_fork_list_dict[subitem].add(item)\n",
    "                    else:\n",
    "                        temp_property_arr = temp_property_arr + property_dict[item]\n",
    "                        \n",
    "                else:\n",
    "                    temp = set([item])\n",
    "                    for subitem in reversed_fork_list_dict[item]:\n",
    "                        temp.update(cur_layer_list[subitem])\n",
    "                    cur_layer_list[item] = temp\n",
    "                    # print(cur_layer_list)\n",
    "                    ## 单路节点\n",
    "                    alloc_dict[Num].append(item)\n",
    "                    have_been_alloc_list[item] = Num\n",
    "                    # added_point.append(item)\n",
    "                    temp_all_nodes_list,temp_extra_node_list,temp_reversed_fork_node_list = find_correct_nodes(cob_G,item,have_been_alloc_list,Num)\n",
    "                    # print(item)\n",
    "                    # print(temp_extra_node_list)\n",
    "                    all_nodes_list += temp_all_nodes_list\n",
    "                    extra_nodes_list += temp_extra_node_list\n",
    "                    \n",
    "                    for subitem in temp_all_nodes_list:\n",
    "                        reversed_fork_list_dict[subitem].add(item)\n",
    "                    \n",
    "                    for subitem in temp_reversed_fork_node_list:\n",
    "                        if subitem in reversed_fork_node_dict:\n",
    "                                reversed_fork_node_dict[subitem] = np.minimum(temp_property_arr,reversed_fork_node_dict[subitem])  \n",
    "                                reversed_fork_list_dict[subitem].add(item)\n",
    "                        else:\n",
    "                            reversed_fork_node_dict[subitem] = temp_property_arr\n",
    "                            reversed_fork_list_dict[subitem].add(item)\n",
    "        # print(temp_all_nodes_list)\n",
    "        if temp_all_nodes_list != []:\n",
    "            temp_temp_all_nodes_list,temp_temp_extra_node_list,temp_property_arr,reversed_fork_node_dict,reversed_fork_list_dict,cur_layer_list = DP_head_tree_recursive_update(temp_property_arr,temp_all_nodes_list,Num,cob_G,reversed_fork_node_dict,reversed_fork_list_dict,cur_layer_list)\n",
    "            extra_nodes_list += temp_temp_extra_node_list\n",
    "            all_nodes_list += temp_temp_all_nodes_list\n",
    "            # added_point = temp_added_point\n",
    "\n",
    "    return all_nodes_list,extra_nodes_list,temp_property_arr,reversed_fork_node_dict,reversed_fork_list_dict,cur_layer_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini_ = 100\n",
    "# for i in tqdm(range(5000)):\n",
    "cur_layer_list = defaultdict(set)\n",
    "acc_limit_arr = np.zeros((10000,4))\n",
    "point_list = [start_point]\n",
    "Num = 0  ## 层级\n",
    "TCM_even = 0  ## 偶数数量\n",
    "alloc_dict = defaultdict(list)\n",
    "have_been_alloc_list = {}\n",
    "TCM_even_list = []\n",
    "all_len = 0\n",
    "while True:\n",
    "    extra_point_list = []\n",
    "    if  16 <= Num and Num<= 31:\n",
    "        temp_property_arr =np.array([1,3,56+acc_limit_arr[Num-16,2],64+acc_limit_arr[Num-16,3]]) - acc_limit_arr[Num-16,:]\n",
    "        orig_arr = temp_property_arr\n",
    "    else:\n",
    "        temp_property_arr = np.array([1,2,56,64])\n",
    "        orig_arr = temp_property_arr\n",
    "    # if Num==29:\n",
    "    #     print(temp_property_arr)\n",
    "    # print(temp_property_arr)\n",
    "    # print(point_list)\n",
    "    # print(have_been_alloc_list)\n",
    "    temp_all_nodes_list,temp_extra_node_list,temp_property_arr,reversed_fork_node_dict,reversed_fork_list_dict,cur_layer_list =DP_head_tree_recursive_update(temp_property_arr,point_list,Num,cob_G,{})\n",
    "   \n",
    "    \n",
    "    \n",
    "    break\n",
    "    # print(point_list)\n",
    "    # print(temp_all_nodes_list)\n",
    "    point_list =  temp_all_nodes_list + temp_extra_node_list + point_list\n",
    "    # print(reversed_fork_node_dict)\n",
    "    # print(point_list)\n",
    "    for item in added_point:\n",
    "        point_list.remove(item)\n",
    "    acc_limit_arr[Num,:] =  orig_arr-temp_property_arr\n",
    "    \n",
    "    \n",
    "    if (Num % 2) == 0 and temp_property_arr[0]== 0 and Num-16 not in TCM_even_list:\n",
    "        TCM_even += 1\n",
    "        TCM_even_list.append(Num)\n",
    "    # print(f'{Num} 级共有 {len(alloc_dict[Num])} 个block')\n",
    "    all_len = all_len + len(alloc_dict[Num])\n",
    "    Num += 1\n",
    "    \n",
    "    if point_list == []:\n",
    "        break\n",
    "        # if Num==29:\n",
    "        #     break\n",
    "    # if Num<mini_:\n",
    "    #     mini_ = Num\n",
    "    #     print(mini_)\n",
    "    #     df5 = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in alloc_dict.items() ]))\n",
    "    #     df5 = df5.transpose()\n",
    "    #     # df4.drop(index=0)\n",
    "    #     df5.to_csv('second_problem_w_greedy_tree.csv', header=False)\n",
    "    #     pd.DataFrame(acc_limit_arr).to_csv(\"first_problem_w_greedy_alloc.csv\", header=False)\n",
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = np.zeros((len(cur_layer_list)+1,temp_property_arr[0]+1,temp_property_arr[1]+1,temp_property_arr[2]+1,temp_property_arr[3]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_cal_dict = defaultdict(lambda: np.zeros(4,dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in cur_layer_list:\n",
    "    for subitem in cur_layer_list[item]:\n",
    "        property_cal_dict[item] += property_dict[subitem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_map_ele = {}\n",
    "cal = 1\n",
    "for item in cur_layer_list:\n",
    "    index_map_ele[cal] = item\n",
    "    cal+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_dict = defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in index_map_ele:\n",
    "    if (property_cal_dict[index_map_ele[item]]<=np.array([0,0,0,0])).all() == True:\n",
    "        dp[(item,0,0,0,0)] = len(cur_layer_list[index_map_ele[item]])\n",
    "        dp_dict[(item,0,0,0,0)] = deepcopy(cur_layer_list[index_map_ele[item]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:03<00:00,  8.72it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1,len(cur_layer_list)+1)):\n",
    "    for j in range(1,temp_property_arr[0]+1):\n",
    "        for k in range(1,temp_property_arr[1]+1):\n",
    "            for l in range(1,temp_property_arr[2]+1):\n",
    "                for m in range(1,temp_property_arr[3]+1):\n",
    "                    dp[i,j,k,l,m] = dp[i-1,j,k,l,m]\n",
    "                    dp_dict[(i,j,k,l,m)] = dp_dict[(i-1,j,k,l,m)]\n",
    "                    if (np.array([j,k,l,m])>=property_cal_dict[index_map_ele[i]]).all() == True:\n",
    "                        check_point = dp_dict[(i-1,j-property_cal_dict[index_map_ele[i]][0],k-property_cal_dict[index_map_ele[i]][1],l-property_cal_dict[index_map_ele[i]][2],m-property_cal_dict[index_map_ele[i]][3])]\n",
    "                        if len(check_point & cur_layer_list[index_map_ele[i]])==0:\n",
    "                            dp[i,j,k,l,m] = max(dp[i,j,k,l,m],dp[i-1,j-property_cal_dict[index_map_ele[i]][0],k-property_cal_dict[index_map_ele[i]][1],l-property_cal_dict[index_map_ele[i]][2],m-property_cal_dict[index_map_ele[i]][3]]+len(cur_layer_list[index_map_ele[i]]))\n",
    "                            dp_dict[(i,j,k,l,m)] = dp_dict[(i-1,j-property_cal_dict[index_map_ele[i]][0],k-property_cal_dict[index_map_ele[i]][1],l-property_cal_dict[index_map_ele[i]][2],m-property_cal_dict[index_map_ele[i]][3])] | cur_layer_list[index_map_ele[i]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('prostate')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2288abd25adbb98e605770a605d977c869a9796aa508dbaffbb6aa07471919db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
